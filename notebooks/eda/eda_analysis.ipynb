{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MUFG DataScience Challenge 2025 - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs exploratory data analysis on the crowdfunding dataset to understand patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Basic Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the training data\ntrain_df = pd.read_csv('../../data/raw/train.csv')\n\nprint(f\"Dataset shape: {train_df.shape}\")\nprint(f\"\\nColumn names:\")\nprint(train_df.columns.tolist())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the dataset\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and basic statistics\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "target_counts = train_df['final_status'].value_counts()\n",
    "print(\"Target variable distribution:\")\n",
    "print(target_counts)\n",
    "print(f\"\\nSuccess rate: {target_counts[1] / len(train_df) * 100:.2f}%\")\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Bar plot\n",
    "target_counts.plot(kind='bar', ax=ax1, color=['coral', 'lightblue'])\n",
    "ax1.set_title('Target Variable Distribution')\n",
    "ax1.set_xlabel('Final Status (0=Failed, 1=Success)')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "ax2.pie(target_counts.values, labels=['Failed (0)', 'Success (1)'], \n",
    "        autopct='%1.1f%%', colors=['coral', 'lightblue'])\n",
    "ax2.set_title('Target Variable Proportion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = train_df.isnull().sum()\n",
    "missing_percentage = (missing_data / len(train_df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(\"Missing values summary:\")\n",
    "    print(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns\n",
    "numerical_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "# Statistical summary of numerical features\n",
    "train_df[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of goal amounts\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(train_df['goal'], bins=50, alpha=0.7, color='skyblue')\n",
    "plt.title('Distribution of Goal Amount')\n",
    "plt.xlabel('Goal Amount')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(np.log1p(train_df['goal']), bins=50, alpha=0.7, color='lightgreen')\n",
    "plt.title('Distribution of Log(Goal Amount + 1)')\n",
    "plt.xlabel('Log(Goal Amount + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot(train_df['goal'])\n",
    "plt.title('Box Plot of Goal Amount')\n",
    "plt.ylabel('Goal Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal amount by success/failure\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "train_df.boxplot(column='goal', by='final_status', ax=plt.gca())\n",
    "plt.title('Goal Amount by Final Status')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for status in [0, 1]:\n",
    "    subset = train_df[train_df['final_status'] == status]\n",
    "    plt.hist(np.log1p(subset['goal']), bins=30, alpha=0.6, \n",
    "             label=f'Status {status}', density=True)\n",
    "plt.title('Log(Goal Amount) Distribution by Status')\n",
    "plt.xlabel('Log(Goal Amount + 1)')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical comparison\n",
    "print(\"Goal amount statistics by final status:\")\n",
    "print(train_df.groupby('final_status')['goal'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp columns to datetime\n",
    "timestamp_cols = ['deadline', 'state_changed_at', 'created_at', 'launched_at']\n",
    "\n",
    "for col in timestamp_cols:\n",
    "    train_df[f'{col}_dt'] = pd.to_datetime(train_df[col], unit='s')\n",
    "\n",
    "# Display sample of converted timestamps\n",
    "print(\"Sample of converted timestamps:\")\n",
    "print(train_df[['deadline', 'deadline_dt', 'created_at', 'created_at_dt']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived temporal features\n",
    "train_df['campaign_duration'] = (train_df['deadline_dt'] - train_df['launched_at_dt']).dt.days\n",
    "train_df['prep_time'] = (train_df['launched_at_dt'] - train_df['created_at_dt']).dt.days\n",
    "\n",
    "# Extract date components\n",
    "train_df['launch_year'] = train_df['launched_at_dt'].dt.year\n",
    "train_df['launch_month'] = train_df['launched_at_dt'].dt.month\n",
    "train_df['launch_day_of_week'] = train_df['launched_at_dt'].dt.dayofweek\n",
    "\n",
    "print(\"Sample of derived temporal features:\")\n",
    "print(train_df[['campaign_duration', 'prep_time', 'launch_year', 'launch_month']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze temporal patterns\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Campaign duration distribution\naxes[0,0].hist(train_df['campaign_duration'], bins=50, alpha=0.7, color='lightcoral')\naxes[0,0].set_title('Campaign Duration Distribution')\naxes[0,0].set_xlabel('Duration (days)')\n\n# Success rate by launch year\nyearly_success = train_df.groupby('launch_year')['final_status'].agg(['count', 'mean']).reset_index()\naxes[0,1].bar(yearly_success['launch_year'], yearly_success['mean'], alpha=0.7, color='lightblue')\naxes[0,1].set_title('Success Rate by Launch Year')\naxes[0,1].set_xlabel('Year')\naxes[0,1].set_ylabel('Success Rate')\n\n# Success rate by launch month\nmonthly_success = train_df.groupby('launch_month')['final_status'].mean()\naxes[1,0].bar(monthly_success.index, monthly_success.values, alpha=0.7, color='lightgreen')\naxes[1,0].set_title('Success Rate by Launch Month')\naxes[1,0].set_xlabel('Month')\naxes[1,0].set_ylabel('Success Rate')\n\n# Success rate by day of week\ndow_success = train_df.groupby('launch_day_of_week')['final_status'].mean()\ndow_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\naxes[1,1].bar(range(7), dow_success.values, alpha=0.7, color='gold')\naxes[1,1].set_title('Success Rate by Day of Week')\naxes[1,1].set_xlabel('Day of Week')\naxes[1,1].set_ylabel('Success Rate')\naxes[1,1].set_xticks(range(7))\naxes[1,1].set_xticklabels(dow_labels)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "source": "## 5.1. Advanced Temporal Analysis - Preparation & Campaign Timing\n\nDeep dive into temporal patterns focusing on preparation time, campaign duration, and state change timing patterns.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Create advanced temporal features\ndef create_advanced_time_features(df):\n    \"\"\"Create advanced temporal features with data cleaning\"\"\"\n    df = df.copy()\n    \n    # Calculate time intervals in days\n    df['preparation_days'] = (df['launched_at'] - df['created_at']) / (24*3600)\n    df['campaign_days'] = (df['deadline'] - df['launched_at']) / (24*3600)  \n    df['state_change_days'] = (df['state_changed_at'] - df['launched_at']) / (24*3600)\n    \n    # Calculate state change ratio (when during campaign the state changed)\n    df['state_change_ratio'] = df['state_change_days'] / df['campaign_days']\n    \n    return df\n\n# Apply advanced feature engineering\nprint(\"Creating advanced temporal features...\")\ntrain_df = create_advanced_time_features(train_df)\n\n# Check for data quality issues (negative time intervals)\nprint(\"\\n=== DATA QUALITY CHECK ===\")\nprint(\"Checking for negative time intervals...\")\n\nnegative_prep = (train_df['preparation_days'] < 0).sum()\nnegative_campaign = (train_df['campaign_days'] < 0).sum()\n\nprint(f\"Projects with negative preparation time: {negative_prep}\")\nprint(f\"Projects with negative campaign duration: {negative_campaign}\")\n\n# Remove projects with negative time intervals\noriginal_size = len(train_df)\ntrain_df = train_df[\n    (train_df['preparation_days'] >= 0) & \n    (train_df['campaign_days'] >= 0)\n].copy()\n\neliminated_count = original_size - len(train_df)\nprint(f\"\\n📊 ELIMINATED {eliminated_count} projects with invalid time intervals\")\nprint(f\"Remaining projects: {len(train_df):,} (was {original_size:,})\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Statistical analysis of temporal features\nprint(\"=== STATISTICAL ANALYSIS OF TEMPORAL FEATURES ===\")\n\ntemporal_features = ['preparation_days', 'campaign_days', 'state_change_days', 'state_change_ratio']\n\nprint(\"\\nOverall Statistics:\")\nstats_df = train_df[temporal_features].describe()\nprint(stats_df.round(2))\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TEMPORAL FEATURES BY SUCCESS STATUS\")\nprint(\"=\"*60)\n\n# Analysis by success status\nfor feature in ['preparation_days', 'campaign_days']:\n    print(f\"\\n📈 {feature.upper().replace('_', ' ')} ANALYSIS:\")\n    \n    # Statistics by success status\n    success_stats = train_df.groupby('final_status')[feature].describe().round(2)\n    print(success_stats)\n    \n    # Success rate by quartiles\n    quartiles = pd.qcut(train_df[feature], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n    quartile_success = train_df.groupby(quartiles)['final_status'].agg(['count', 'mean']).round(3)\n    quartile_success.columns = ['Project_Count', 'Success_Rate']\n    print(f\"\\nSuccess rate by {feature} quartiles:\")\n    print(quartile_success)\n    print(\"-\" * 40)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Investigate state_changed_at mystery\nprint(\"=== STATE_CHANGED_AT MYSTERY INVESTIGATION ===\")\n\n# Check if state_changed_at equals deadline for failed projects\nfailed_projects = train_df[train_df['final_status'] == 0]\nsuccessful_projects = train_df[train_df['final_status'] == 1]\n\n# Check state_changed_at == deadline\nfailed_state_eq_deadline = (failed_projects['state_changed_at'] == failed_projects['deadline']).sum()\nsuccessful_state_eq_deadline = (successful_projects['state_changed_at'] == successful_projects['deadline']).sum()\n\nprint(f\"\\n🔍 HYPOTHESIS: state_changed_at == deadline for failed projects\")\nprint(f\"Failed projects where state_changed_at == deadline: {failed_state_eq_deadline:,} / {len(failed_projects):,} ({failed_state_eq_deadline/len(failed_projects)*100:.1f}%)\")\nprint(f\"Successful projects where state_changed_at == deadline: {successful_state_eq_deadline:,} / {len(successful_projects):,} ({successful_state_eq_deadline/len(successful_projects)*100:.1f}%)\")\n\n# Analyze state change timing patterns\nprint(f\"\\n📊 STATE CHANGE TIMING PATTERNS:\")\nprint(f\"Failed projects - Average state change ratio: {failed_projects['state_change_ratio'].mean():.3f}\")\nprint(f\"Successful projects - Average state change ratio: {successful_projects['state_change_ratio'].mean():.3f}\")\n\n# Check for early vs late state changes\nearly_change = train_df[train_df['state_change_ratio'] <= 0.5]\nlate_change = train_df[train_df['state_change_ratio'] > 0.5]\n\nprint(f\"\\n⏰ EARLY vs LATE STATE CHANGES:\")\nprint(f\"Early changes (≤50% of campaign): {len(early_change):,} projects, Success rate: {early_change['final_status'].mean():.3f}\")\nprint(f\"Late changes (>50% of campaign): {len(late_change):,} projects, Success rate: {late_change['final_status'].mean():.3f}\")\n\n# Special cases analysis\nstate_change_at_launch = (train_df['state_change_days'] == 0).sum()\nstate_change_after_deadline = (train_df['state_change_days'] > train_df['campaign_days']).sum()\n\nprint(f\"\\n🚨 SPECIAL CASES:\")\nprint(f\"State changed at launch (day 0): {state_change_at_launch:,} projects\")\nprint(f\"State changed after deadline: {state_change_after_deadline:,} projects\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize temporal patterns and success rates\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\n\n# 1. Preparation days distribution\naxes[0,0].hist(train_df['preparation_days'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\naxes[0,0].set_title('Preparation Days Distribution')\naxes[0,0].set_xlabel('Preparation Days')\naxes[0,0].set_ylabel('Frequency')\naxes[0,0].axvline(train_df['preparation_days'].median(), color='red', linestyle='--', label=f'Median: {train_df[\"preparation_days\"].median():.1f}')\naxes[0,0].legend()\n\n# 2. Campaign days distribution  \naxes[0,1].hist(train_df['campaign_days'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\naxes[0,1].set_title('Campaign Days Distribution')\naxes[0,1].set_xlabel('Campaign Days')\naxes[0,1].set_ylabel('Frequency')\naxes[0,1].axvline(train_df['campaign_days'].median(), color='red', linestyle='--', label=f'Median: {train_df[\"campaign_days\"].median():.1f}')\naxes[0,1].legend()\n\n# 3. State change ratio distribution\naxes[0,2].hist(train_df['state_change_ratio'], bins=50, alpha=0.7, color='coral', edgecolor='black')\naxes[0,2].set_title('State Change Ratio Distribution')\naxes[0,2].set_xlabel('State Change Ratio (0=start, 1=end)')\naxes[0,2].set_ylabel('Frequency')\naxes[0,2].axvline(train_df['state_change_ratio'].median(), color='red', linestyle='--', label=f'Median: {train_df[\"state_change_ratio\"].median():.2f}')\naxes[0,2].legend()\n\n# 4. Success rate by preparation days (binned)\nprep_bins = pd.cut(train_df['preparation_days'], bins=15)\nprep_success = train_df.groupby(prep_bins)['final_status'].agg(['mean', 'count'])\nprep_success = prep_success[prep_success['count'] >= 50]  # Only bins with sufficient data\n\nx_prep = range(len(prep_success))\naxes[1,0].bar(x_prep, prep_success['mean'], alpha=0.7, color='skyblue')\naxes[1,0].set_title('Success Rate by Preparation Days')\naxes[1,0].set_xlabel('Preparation Days (binned)')\naxes[1,0].set_ylabel('Success Rate')\naxes[1,0].set_xticks(x_prep[::2])  # Show every 2nd label to avoid crowding\naxes[1,0].tick_params(axis='x', rotation=45)\n\n# 5. Success rate by campaign days (binned)\ncampaign_bins = pd.cut(train_df['campaign_days'], bins=15)\ncampaign_success = train_df.groupby(campaign_bins)['final_status'].agg(['mean', 'count'])\ncampaign_success = campaign_success[campaign_success['count'] >= 50]  # Only bins with sufficient data\n\nx_campaign = range(len(campaign_success))\naxes[1,1].bar(x_campaign, campaign_success['mean'], alpha=0.7, color='lightgreen')\naxes[1,1].set_title('Success Rate by Campaign Days')\naxes[1,1].set_xlabel('Campaign Days (binned)')\naxes[1,1].set_ylabel('Success Rate')\naxes[1,1].set_xticks(x_campaign[::2])\naxes[1,1].tick_params(axis='x', rotation=45)\n\n# 6. Success rate by state change timing\nstate_bins = pd.cut(train_df['state_change_ratio'], bins=10)\nstate_success = train_df.groupby(state_bins)['final_status'].agg(['mean', 'count'])\nstate_success = state_success[state_success['count'] >= 30]\n\nx_state = range(len(state_success))\naxes[1,2].bar(x_state, state_success['mean'], alpha=0.7, color='coral')\naxes[1,2].set_title('Success Rate by State Change Timing')\naxes[1,2].set_xlabel('State Change Ratio (binned)')\naxes[1,2].set_ylabel('Success Rate')\naxes[1,2].set_xticks(x_state)\naxes[1,2].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Advanced temporal insights summary\nprint(\"=\" * 70)\nprint(\"🕰️  ADVANCED TEMPORAL ANALYSIS - KEY FINDINGS\")\nprint(\"=\" * 70)\n\n# Optimal ranges analysis\nprep_quartile_success = train_df.groupby(pd.qcut(train_df['preparation_days'], q=4))['final_status'].mean()\ncampaign_quartile_success = train_df.groupby(pd.qcut(train_df['campaign_days'], q=4))['final_status'].mean()\n\nbest_prep_quartile = prep_quartile_success.idxmax()\nbest_campaign_quartile = campaign_quartile_success.idxmax()\n\nprint(f\"\\n📈 OPTIMAL TIMING PATTERNS:\")\nprint(f\"   Best preparation quartile: {best_prep_quartile} (Success rate: {prep_quartile_success.max():.3f})\")\nprint(f\"   Best campaign quartile: {best_campaign_quartile} (Success rate: {campaign_quartile_success.max():.3f})\")\n\n# Extreme cases analysis\nvery_short_prep = train_df[train_df['preparation_days'] <= 1]['final_status'].mean()\nvery_long_prep = train_df[train_df['preparation_days'] >= 180]['final_status'].mean()\nvery_short_campaign = train_df[train_df['campaign_days'] <= 15]['final_status'].mean()\nvery_long_campaign = train_df[train_df['campaign_days'] >= 90]['final_status'].mean()\n\nprint(f\"\\n⚠️  EXTREME CASES ANALYSIS:\")\nprint(f\"   Very short prep (≤1 day): {very_short_prep:.3f} success rate\")\nprint(f\"   Very long prep (≥180 days): {very_long_prep:.3f} success rate\")\nprint(f\"   Very short campaign (≤15 days): {very_short_campaign:.3f} success rate\")\nprint(f\"   Very long campaign (≥90 days): {very_long_campaign:.3f} success rate\")\n\n# State change insights\nfailed_at_deadline_pct = failed_state_eq_deadline / len(failed_projects) * 100\nsuccessful_at_deadline_pct = successful_state_eq_deadline / len(successful_projects) * 100\n\nprint(f\"\\n🔍 STATE_CHANGED_AT MYSTERY SOLVED:\")\nprint(f\"   Failed projects ending at deadline: {failed_at_deadline_pct:.1f}%\")\nprint(f\"   Successful projects ending at deadline: {successful_at_deadline_pct:.1f}%\")\nprint(f\"   → Hypothesis CONFIRMED: Failed projects often end exactly at deadline\")\n\nprint(f\"\\n💡 FEATURE ENGINEERING RECOMMENDATIONS:\")\nprint(f\"   1. Create preparation_days categories (short/medium/long)\")\nprint(f\"   2. Create campaign_days categories (optimal 30-60 day range)\")\nprint(f\"   3. Use state_change_ratio as success predictor\")\nprint(f\"   4. Flag projects ending exactly at deadline (failure indicator)\")\nprint(f\"   5. Create 'rushed' projects feature (very short prep time)\")\n\nprint(f\"\\n✅ Analysis completed with {len(train_df):,} projects\")",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Categorical Features Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Identify categorical columns\ncategorical_cols = ['country', 'currency', 'disable_communication']\n\n# Analyze country distribution\ncountry_stats = train_df.groupby('country').agg({\n    'final_status': ['count', 'mean']\n}).round(3)\ncountry_stats.columns = ['Project_Count', 'Success_Rate']\ncountry_stats = country_stats.sort_values('Project_Count', ascending=False)\n\nprint(\"Top 15 countries by project count:\")\nprint(country_stats.head(15))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize country analysis\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Top countries by project count\ntop_countries = country_stats.head(10)\naxes[0].bar(range(len(top_countries)), top_countries['Project_Count'], alpha=0.7)\naxes[0].set_title('Top 10 Countries by Project Count')\naxes[0].set_xlabel('Country')\naxes[0].set_ylabel('Project Count')\naxes[0].set_xticks(range(len(top_countries)))\naxes[0].set_xticklabels(top_countries.index, rotation=45)\n\n# Success rate for top countries\naxes[1].bar(range(len(top_countries)), top_countries['Success_Rate'], alpha=0.7, color='orange')\naxes[1].set_title('Success Rate for Top 10 Countries')\naxes[1].set_xlabel('Country')\naxes[1].set_ylabel('Success Rate')\naxes[1].set_xticks(range(len(top_countries)))\naxes[1].set_xticklabels(top_countries.index, rotation=45)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze currency distribution\ncurrency_stats = train_df.groupby('currency').agg({\n    'final_status': ['count', 'mean']\n}).round(3)\ncurrency_stats.columns = ['Project_Count', 'Success_Rate']\ncurrency_stats = currency_stats.sort_values('Project_Count', ascending=False)\n\nprint(\"Currency distribution:\")\nprint(currency_stats.head(10))\n\n# Analyze disable_communication feature\ncomm_stats = train_df.groupby('disable_communication')['final_status'].agg(['count', 'mean'])\nprint(\"\\nCommunication setting analysis:\")\nprint(comm_stats)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Text Features Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text features: name, desc, keywords\n",
    "text_cols = ['name', 'desc', 'keywords']\n",
    "\n",
    "# Calculate text lengths\n",
    "for col in text_cols:\n",
    "    train_df[f'{col}_length'] = train_df[col].str.len()\n",
    "    train_df[f'{col}_word_count'] = train_df[col].str.split().str.len()\n",
    "\n",
    "# Analyze text statistics by success\n",
    "text_stats = train_df.groupby('final_status')[['name_length', 'desc_length', 'keywords_length']].mean()\n",
    "print(\"Average text lengths by success status:\")\n",
    "print(text_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, col in enumerate(['name_length', 'desc_length', 'keywords_length']):\n",
    "    for status in [0, 1]:\n",
    "        subset = train_df[train_df['final_status'] == status]\n",
    "        axes[i].hist(subset[col], bins=30, alpha=0.6, label=f'Status {status}', density=True)\n",
    "    \n",
    "    axes[i].set_title(f'{col.replace(\"_\", \" \").title()} Distribution')\n",
    "    axes[i].set_xlabel('Length')\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for correlation analysis\n",
    "corr_features = ['goal', 'final_status', 'campaign_duration', 'prep_time', \n",
    "                'launch_year', 'launch_month', 'launch_day_of_week',\n",
    "                'name_length', 'desc_length', 'keywords_length']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = train_df[corr_features].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlations with target variable\n",
    "target_corr = corr_matrix['final_status'].sort_values(ascending=False)\n",
    "print(\"Correlations with target variable (final_status):\")\n",
    "print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== KEY INSIGHTS FROM EDA ===\")\nprint(f\"\\n1. Dataset Overview:\")\nprint(f\"   - Total projects: {len(train_df):,}\")\nprint(f\"   - Success rate: {train_df['final_status'].mean()*100:.1f}%\")\nprint(f\"   - Features: {train_df.shape[1]} columns\")\n\nprint(f\"\\n2. Target Variable:\")\nsuccess_rate = train_df['final_status'].mean() * 100\nif success_rate < 40:\n    balance_note = \"Highly imbalanced - consider sampling techniques\"\nelif success_rate < 45:\n    balance_note = \"Moderately imbalanced\"\nelse:\n    balance_note = \"Relatively balanced\"\nprint(f\"   - {balance_note}\")\n\nprint(f\"\\n3. Goal Amount:\")\ngoal_stats = train_df['goal'].describe()\nprint(f\"   - Median goal: ${goal_stats['50%']:,.0f}\")\nprint(f\"   - Mean goal: ${goal_stats['mean']:,.0f}\")\nprint(f\"   - High variance (skewed distribution)\")\n\nprint(f\"\\n4. Temporal Patterns:\")\nprint(f\"   - Average campaign duration: {train_df['campaign_duration'].mean():.0f} days\")\nprint(f\"   - Launch years: {train_df['launch_year'].min()}-{train_df['launch_year'].max()}\")\n\nprint(f\"\\n5. Geographic Distribution:\")\ntop_country = country_stats.index[0]\ntop_country_pct = (country_stats.iloc[0]['Project_Count'] / len(train_df)) * 100\nprint(f\"   - Top country: {top_country} ({top_country_pct:.1f}% of projects)\")\nprint(f\"   - Number of countries: {train_df['country'].nunique()}\")\n\nprint(f\"\\n6. Strongest Predictors (correlation with success):\")\ntop_corr = target_corr.drop('final_status').head(3)\nfor feature, corr in top_corr.items():\n    print(f\"   - {feature}: {corr:.3f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}